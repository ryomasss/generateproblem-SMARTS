#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AI Dataset Annotation Tool
ç”¨äºäººå·¥æ ‡æ³¨ AI æ‹¦æˆªçš„ååº”ï¼Œç”Ÿæˆå¾®è°ƒæ•°æ®é›†ã€‚
"""

import json
import os
import sys

# Paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FAILED_FILE = os.path.join(BASE_DIR, 'data', 'failed_reactions.json')
TRAIN_FILE = os.path.join(BASE_DIR, 'data', 'training_data.jsonl')

def load_failed():
    if not os.path.exists(FAILED_FILE):
        return []
    try:
        with open(FAILED_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return []

def save_training_data(entry):
    """Save an entry to JSONL format"""
    with open(TRAIN_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(entry, ensure_ascii=False) + '\n')

def main():
    print("=" * 60)
    print("      ğŸ§ª AI ååº”æ•°æ®æ ‡æ³¨å·¥å…· (Data Annotator)      ")
    print("=" * 60)
    
    entries = load_failed()
    
    if not entries:
        print("\n[Info] æ²¡æœ‰å‘ç°è¢«æ‹¦æˆªçš„ååº”è®°å½•ã€‚")
        return

    print(f"\nå‘ç° {len(entries)} æ¡å¾…å¤„ç†è®°å½•ã€‚")
    print("æ“ä½œé€»è¾‘ï¼š")
    print(" [y] - åˆç† (AI åˆ¤é”™äº†ï¼Œè¿™æ˜¯ä¸ªæ­£ç¡®çš„åŒ–å­¦ååº”)")
    print(" [n] - ä¸åˆç† (AI åˆ¤å¯¹äº†ï¼Œè¿™ä¸ªäº§ç‰©å†™é”™äº†)")
    print(" [s] - è·³è¿‡ (ä¸ç¡®å®š)")
    print(" [q] - é€€å‡ºå¹¶ä¿å­˜\n")

    remaining = []
    processed_count = 0

    for i, entry in enumerate(entries):
        print("-" * 50)
        print(f"æ¡ç›® {i+1}/{len(entries)}")
        print(f"ååº”ç‰©: {'.'.join(entry['reactants'])}")
        print(f"äº§ç‰©:   {entry['product']}")
        print(f"AI è¯„åˆ†: {entry['similarity']} (åŸå› : {entry['reason']})")
        
        while True:
            choice = input("\nè¯¥ååº”æ˜¯å¦åˆç†? [y/n/s/q]: ").lower()
            
            if choice == 'y':
                # è®°å½•ä¸ºæ­£ç¡® (1)
                save_training_data({
                    "reactants": ".".join(entry['reactants']),
                    "product": entry['product'],
                    "label": 1,
                    "smarts": entry.get('smarts')
                })
                processed_count += 1
                break
            elif choice == 'n':
                # è®°å½•ä¸ºé”™è¯¯ (0)
                save_training_data({
                    "reactants": ".".join(entry['reactants']),
                    "product": entry['product'],
                    "label": 0,
                    "smarts": entry.get('smarts')
                })
                processed_count += 1
                break
            elif choice == 's':
                remaining.append(entry)
                break
            elif choice == 'q':
                # é€€å‡ºå‰ä¿å­˜æœªå¤„ç†çš„
                remaining.extend(entries[i:])
                break
            else:
                print("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ y, n, s æˆ– q")
        
        if choice == 'q':
            break

    # æ›´æ–°å¤±è´¥è®°å½•æ–‡ä»¶ï¼Œç§»é™¤å·²å¤„ç†çš„
    with open(FAILED_FILE, 'w', encoding='utf-8') as f:
        json.dump(remaining, f, indent=2, ensure_ascii=False)

    print(f"\næ ‡æ³¨å®Œæˆï¼")
    print(f"æœ¬æ¬¡æ ‡æ³¨: {processed_count} æ¡")
    print(f"å‰©ä½™å¾…å¤„ç†: {len(remaining)} æ¡")
    print(f"æ•°æ®é›†ä½ç½®: {TRAIN_FILE}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\næ“ä½œå·²å–æ¶ˆã€‚")
